{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838dbe82-e8b2-40f8-8324-993cd07b1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6019c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to run this command as an administrator\n",
    "#pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4ae122-1e3f-4f89-b794-2e2c6557d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logging\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d2f334-07e7-46bf-8723-c193c361a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['OPEN', 'HIGH', 'LOW', 'PREV. CLOSE', 'ltp', 'close', 'vwap', '52W H', '52W L', 'VOLUME', 'VALUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a06ac6-8a3c-4af2-8449-fde32a979761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the action space\n",
    "actions = ['BUY', 'SELL', 'HOLD']\n",
    "num_actions = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85204b12-397b-458d-a178-f9e0a79cdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DDQL agent class\n",
    "class DDQLAgent:\n",
    "    def __init__(self, state_size, action_size, look_back):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1500)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_decay = 0.990\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001\n",
    "        self.look_back = look_back  # Number of previous time steps to consider\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential([\n",
    "            LSTM(32, input_shape=(self.look_back, self.state_size), return_sequences=True, \n",
    "                 kernel_regularizer=regularizers.l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32, return_sequences=False, kernel_regularizer=regularizers.l2(0.01)),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "            Dense(self.action_size, activation='linear', kernel_regularizer=regularizers.l2(0.01))\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, env):\n",
    "        state = np.reshape(state, [1, self.look_back, self.state_size])\n",
    "        if env.current_step == env.look_back:\n",
    "            return 0  # Buy action at the start of each episode\n",
    "        else:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                return random.randint(0, self.action_size - 1)\n",
    "            else:\n",
    "                act_values = self.model.predict(state, verbose=0)\n",
    "                return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state, verbose=0)[0]))\n",
    "            target_f = self.model.predict(state, verbose=0)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5c1a5a-70ef-42ab-bddb-f2bc7d617acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Environment\n",
    "\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, data, look_back):\n",
    "        self.data = data\n",
    "        self.n = len(data)\n",
    "        self.current_step = 0\n",
    "        self.hold_counter = 0\n",
    "        self.hold_penalty = 1  # Initialize the hold penalty factor\n",
    "        self.look_back = look_back  # Add look_back attribute\n",
    "        self.buy_price = -1\n",
    "        self.has_bought = False  # Track if the agent has already bought the stock\n",
    "        self.is_holding = False  # New state variable to track holding position\n",
    "        #self.actions_taken = []\n",
    "\n",
    "    def is_illegal_action(self, action):\n",
    "        # Example condition: cannot sell if not holding a stock\n",
    "        if actions[action] == 'SELL' and self.buy_price == -1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def detect_bullish_engulfing(self):\n",
    "        if self.current_step < 1:\n",
    "            return False\n",
    "    \n",
    "        previous_candle = self.data.iloc[self.current_step - 1]\n",
    "        current_candle = self.data.iloc[self.current_step]\n",
    "    \n",
    "        if (previous_candle['close'] < previous_candle['OPEN'] and\n",
    "                current_candle['close'] > current_candle['OPEN'] and\n",
    "                current_candle['close'] > previous_candle['OPEN'] and\n",
    "                current_candle['OPEN'] < previous_candle['close']):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.look_back  # Start from look_back step\n",
    "        self.buy_price = -1  # Reset buy_price to indicate no stock held\n",
    "        self.is_holding = False  # Reset holding position\n",
    "        self.has_bought = False  # Reset the flag when the episode resets\n",
    "\n",
    "        return self.data.iloc[self.current_step - self.look_back:self.current_step][features].values\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = np.zeros((self.look_back, len(features)))\n",
    "\n",
    "        if self.is_illegal_action(action):\n",
    "            # Handle illegal action\n",
    "            reward = -100  # Penalty for illegal action\n",
    "        else:\n",
    "            # Regular action handling\n",
    "            reward = self.calculate_reward(action)\n",
    "\n",
    "        # Increment the step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step + self.look_back > self.n\n",
    "\n",
    "        if not done:\n",
    "            next_state = self.data.iloc[self.current_step:self.current_step + self.look_back][features].values\n",
    "        else:\n",
    "            next_state = np.zeros((self.look_back, len(features)))  # End of data handling\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "    \n",
    "    def calculate_reward(self, action):\n",
    "        if actions[action] == 'SELL':\n",
    "            if self.buy_price == -1:\n",
    "                return -150  # Penalty for selling without holding\n",
    "            else:\n",
    "                profit = self.data.iloc[self.current_step][\"close\"] - self.buy_price\n",
    "                self.buy_price = -1\n",
    "                self.is_holding = False\n",
    "                self.hold_counter = 0\n",
    "                self.has_bought = False\n",
    "                if profit > 0:\n",
    "                    return max(200, profit)  # Reward at least 200 or actual profit if greater\n",
    "                else:\n",
    "                    return profit  # Penalize for loss\n",
    "                \n",
    "        elif actions[action] == 'BUY':\n",
    "            if self.is_holding:\n",
    "                return -200  # Increased penalty for trying to buy when already holding\n",
    "            else:\n",
    "                self.buy_price = self.data.iloc[self.current_step][\"OPEN\"]\n",
    "                self.is_holding = True\n",
    "                self.has_bought = True\n",
    "                return 0  # No immediate reward for buying\n",
    "\n",
    "        elif actions[action] == 'HOLD':\n",
    "            if self.is_holding:\n",
    "                potential_profit = self.data.iloc[self.current_step][\"close\"] - self.buy_price\n",
    "                if potential_profit > 0:\n",
    "                    return 10  # Reward for holding a potentially profitable stock\n",
    "                else:\n",
    "                    return -5  # Penalty for holding a non-profitable stock\n",
    "            else:\n",
    "                return -100  # Penalty for holding without buying\n",
    "\n",
    "        else:\n",
    "            return 0  # Default case: no reward or penalty for other actions\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "012b0426-1562-4eb2-8067-bcb0b5c04e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Reward Plot\n",
    "def plot_cumulative_reward(cumulative_rewards):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cumulative_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Cumulative Reward over Episodes')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Action Distribution Plot\n",
    "def plot_action_distribution(all_actions):\n",
    "    actions_flat = [action for actions_episode in all_actions for action in actions_episode]\n",
    "    action_counts = np.unique(actions_flat, return_counts=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(action_counts[0], action_counts[1])\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Action Distribution over Episodes')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Episode Length Plot\n",
    "def plot_episode_lengths(episode_lengths):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(episode_lengths)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode Length')\n",
    "    plt.title('Episode Length over Episodes')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Reward Distribution Plot\n",
    "def plot_reward_distribution(rewards):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(rewards, bins=20)\n",
    "    plt.xlabel('Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reward Distribution over Episodes')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Action Trajectory Plot\n",
    "def plot_action_trajectory(actions_episode):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actions_episode)\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Action')\n",
    "    plt.title('Action Trajectory in a Single Episode')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# State Trajectory Plot\n",
    "def plot_state_trajectory(states_episode):\n",
    "    num_features = states_episode[0].shape[2]  # Get the number of features from the state shape\n",
    "    num_time_steps = len(states_episode)\n",
    "    fig, axs = plt.subplots(num_features, 1, figsize=(10, num_features * 3))\n",
    "    if num_features == 1:\n",
    "        axs = [axs]  # Convert to a list if there's only one feature\n",
    "    for i in range(num_features):\n",
    "        feature_values = [state[0][0][i] for state in states_episode]  # Extract the feature values\n",
    "        axs[i].plot(feature_values)\n",
    "        axs[i].set_xlabel('Time Step')\n",
    "        axs[i].set_ylabel(f'Feature {i+1}')\n",
    "        axs[i].set_title(f'State Feature {i+1} Trajectory in a Single Episode')\n",
    "        axs[i].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Portfolio Value Plot\n",
    "def plot_portfolio_value(portfolio_values):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(portfolio_values)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    plt.title('Portfolio Value over Episodes')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# State-Action Value Heatmap\n",
    "def plot_q_value_heatmap(q_values):\n",
    "    # Assuming q_values is a 2D array where rows represent states and columns represent actions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(q_values, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('State')\n",
    "    plt.title('State-Action Value (Q-Value) Heatmap')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1a5a81-ae4b-4d19-a3cd-8a131e999088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "data = pd.read_csv('fin_data.csv')\n",
    "data = data.iloc[:, 2:] \n",
    "\n",
    "# Define a function to preprocess the data\n",
    "def preprocess_data(data):\n",
    "    # Replace commas with empty strings and convert to float\n",
    "    data = data.replace(',', '', regex=True).astype(float)\n",
    "    return data\n",
    "\n",
    "# Preprocess the entire DataFrame\n",
    "data = preprocess_data(data)\n",
    "\n",
    "look_back = 15  # Assuming you want to look back 10 time steps, adjust as needed\n",
    "state_size = len(features)\n",
    "num_actions = len(actions)\n",
    "\n",
    "# Initialize the environment and agent\n",
    "env = TradingEnvironment(data, look_back)\n",
    "agent = DDQLAgent(state_size, num_actions, look_back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6c84846-b7a0-44a6-a679-9ba790e283f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/250, Total Reward: -257912.95, Total Profit: 4568.00\n",
      "Episode: 2/250, Total Reward: -259841.45, Total Profit: 1422.80\n",
      "Episode: 3/250, Total Reward: -250397.35, Total Profit: 4047.95\n",
      "Episode: 4/250, Total Reward: -252483.45, Total Profit: -4554.75\n",
      "Episode: 5/250, Total Reward: -257785.35, Total Profit: -7.90\n",
      "Episode: 6/250, Total Reward: -260842.60, Total Profit: 7112.05\n",
      "Episode: 7/250, Total Reward: -264456.85, Total Profit: 9842.40\n",
      "Episode: 8/250, Total Reward: -254584.95, Total Profit: 6891.75\n",
      "Episode: 9/250, Total Reward: -261289.70, Total Profit: 8625.25\n",
      "Episode: 10/250, Total Reward: -263938.80, Total Profit: 9611.65\n",
      "Episode: 11/250, Total Reward: -259090.55, Total Profit: 2880.15\n",
      "Episode: 12/250, Total Reward: -255669.45, Total Profit: -4104.15\n",
      "Episode: 13/250, Total Reward: -260589.80, Total Profit: 11820.60\n",
      "Episode: 14/250, Total Reward: -260325.05, Total Profit: 1507.60\n",
      "Episode: 15/250, Total Reward: -259705.50, Total Profit: -2749.10\n",
      "Episode: 16/250, Total Reward: -256822.30, Total Profit: -1428.40\n",
      "Episode: 17/250, Total Reward: -253959.15, Total Profit: 4920.75\n",
      "Episode: 18/250, Total Reward: -246371.55, Total Profit: 12397.10\n",
      "Episode: 19/250, Total Reward: -245719.35, Total Profit: 6323.65\n",
      "Episode: 20/250, Total Reward: -271447.00, Total Profit: -1798.60\n",
      "Episode: 21/250, Total Reward: -254985.95, Total Profit: -3788.70\n",
      "Episode: 22/250, Total Reward: -254160.60, Total Profit: 7756.35\n",
      "Episode: 23/250, Total Reward: -261367.65, Total Profit: 2200.70\n",
      "Episode: 24/250, Total Reward: -252207.90, Total Profit: 532.55\n",
      "Episode: 25/250, Total Reward: -265809.95, Total Profit: 5715.45\n",
      "Episode: 26/250, Total Reward: -256011.10, Total Profit: 11805.90\n",
      "Episode: 27/250, Total Reward: -250951.15, Total Profit: 5936.45\n",
      "Episode: 28/250, Total Reward: -258837.30, Total Profit: 3744.15\n",
      "Episode: 29/250, Total Reward: -256030.35, Total Profit: 2682.85\n",
      "Episode: 30/250, Total Reward: -257782.80, Total Profit: -1831.00\n",
      "Episode: 31/250, Total Reward: -264492.40, Total Profit: 13942.40\n",
      "Episode: 32/250, Total Reward: -260347.20, Total Profit: 10007.50\n",
      "Episode: 33/250, Total Reward: -262431.15, Total Profit: 19874.60\n",
      "Episode: 34/250, Total Reward: -264760.00, Total Profit: -5901.20\n",
      "Episode: 35/250, Total Reward: -258417.65, Total Profit: -2484.20\n",
      "Episode: 36/250, Total Reward: -263719.35, Total Profit: 18581.50\n",
      "Episode: 37/250, Total Reward: -259454.60, Total Profit: 19803.60\n",
      "Episode: 38/250, Total Reward: -253974.80, Total Profit: 17187.65\n",
      "Episode: 39/250, Total Reward: -268761.30, Total Profit: 9550.05\n",
      "Episode: 40/250, Total Reward: -276533.45, Total Profit: 12887.10\n",
      "Episode: 41/250, Total Reward: -277297.20, Total Profit: 6793.50\n",
      "Episode: 42/250, Total Reward: -271846.50, Total Profit: 974.35\n",
      "Episode: 43/250, Total Reward: -272181.40, Total Profit: 11146.30\n",
      "Episode: 44/250, Total Reward: -262609.95, Total Profit: 13995.65\n",
      "Episode: 45/250, Total Reward: -278595.85, Total Profit: -767.30\n",
      "Episode: 46/250, Total Reward: -269224.45, Total Profit: 9095.05\n",
      "Episode: 47/250, Total Reward: -276741.40, Total Profit: 18089.80\n",
      "Episode: 48/250, Total Reward: -270762.30, Total Profit: -5830.60\n",
      "Episode: 49/250, Total Reward: -275809.15, Total Profit: 10523.65\n",
      "Episode: 50/250, Total Reward: -280386.80, Total Profit: 3325.55\n",
      "Episode: 51/250, Total Reward: -280321.45, Total Profit: 18172.70\n",
      "Episode: 52/250, Total Reward: -272799.75, Total Profit: 6566.05\n",
      "Episode: 53/250, Total Reward: -275974.50, Total Profit: 13083.75\n",
      "Episode: 54/250, Total Reward: -264536.35, Total Profit: -22085.80\n",
      "Episode: 55/250, Total Reward: -277737.95, Total Profit: 10400.15\n",
      "Episode: 56/250, Total Reward: -290481.90, Total Profit: 12245.90\n",
      "Episode: 57/250, Total Reward: -290793.20, Total Profit: 15294.80\n",
      "Episode: 58/250, Total Reward: -288912.80, Total Profit: 25188.95\n",
      "Episode: 59/250, Total Reward: -285327.40, Total Profit: 27180.55\n",
      "Episode: 60/250, Total Reward: -303821.70, Total Profit: 18606.80\n",
      "Episode: 61/250, Total Reward: -295935.90, Total Profit: 5582.30\n",
      "Episode: 62/250, Total Reward: -287778.20, Total Profit: 10809.80\n",
      "Episode: 63/250, Total Reward: -298949.70, Total Profit: 25769.95\n",
      "Episode: 64/250, Total Reward: -302214.15, Total Profit: 4333.00\n",
      "Episode: 65/250, Total Reward: -290821.80, Total Profit: 3133.20\n",
      "Episode: 66/250, Total Reward: -292544.85, Total Profit: -4611.95\n",
      "Episode: 67/250, Total Reward: -314641.35, Total Profit: 26514.35\n",
      "Episode: 68/250, Total Reward: -307796.95, Total Profit: 20273.50\n",
      "Episode: 69/250, Total Reward: -488072.15, Total Profit: 1119.80\n",
      "Episode: 70/250, Total Reward: -306509.25, Total Profit: 3495.65\n",
      "Episode: 71/250, Total Reward: -293125.65, Total Profit: -504.60\n",
      "Episode: 72/250, Total Reward: -306275.40, Total Profit: 25474.20\n",
      "Episode: 73/250, Total Reward: -305344.95, Total Profit: 29066.50\n",
      "Episode: 74/250, Total Reward: -300964.25, Total Profit: 6225.70\n",
      "Episode: 75/250, Total Reward: -311578.50, Total Profit: -9655.95\n",
      "Episode: 76/250, Total Reward: -317509.30, Total Profit: -5910.90\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_episodes = 250\n",
    "\n",
    "cumulative_rewards = []\n",
    "episode_lengths = []\n",
    "rewards = []\n",
    "all_actions = []\n",
    "states_episode_all = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, agent.look_back, state_size])\n",
    "    total_reward = 0\n",
    "    episode_length = 0\n",
    "    actions_episode = []\n",
    "    states_episode = []\n",
    "    holding_start_step = None\n",
    "    holding_price = None\n",
    "    holding_steps = 0\n",
    "    episode_profit = 0  # Initialize episode profit\n",
    "    \n",
    "    for t in range(env.n):\n",
    "        action = agent.act(state, env)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        next_state = np.reshape(next_state, [1, agent.look_back, state_size])  # Correct reshaping\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        actions_episode.append(actions[action])\n",
    "        states_episode.append(state)\n",
    "        episode_length += 1\n",
    "        \n",
    "        if actions[action] == 'BUY':\n",
    "            holding_start_step = t\n",
    "            holding_price = env.data.iloc[env.current_step][\"close\"]\n",
    "            holding_steps = 0\n",
    "            #print(f\"Episode: {episode + 1}/{num_episodes}, Step: {t}, Action: BUY, Buy Price: {holding_price:.2f}\")\n",
    "            \n",
    "        elif actions[action] == 'SELL':\n",
    "            if holding_start_step is not None:\n",
    "                buy_price = holding_price\n",
    "                sell_price = env.data.iloc[env.current_step][\"close\"]\n",
    "                profit = sell_price - buy_price\n",
    "                episode_profit += profit  # Accumulate profit for the episode\n",
    "                #print(f\"Episode: {episode + 1}/{num_episodes}, Step: {t}, Action: SELL, Buy Price: {buy_price:.2f}, Sell Price: {sell_price:.2f}, Profit: {profit:.2f}\")\n",
    "        \n",
    "        elif actions[action] == 'HOLD':\n",
    "            if holding_start_step is not None:\n",
    "                holding_steps += 1\n",
    "                #print(f\"Episode: {episode + 1}/{num_episodes}, Step: {t}, Action: HOLD, Holding Price: {holding_price:.2f}, Holding Steps: {holding_steps}\")\n",
    "            \n",
    "        if done:\n",
    "            all_actions.append(actions_episode)\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode: {episode + 1}/{num_episodes}, Total Reward: {total_reward:.2f}, Total Profit: {episode_profit:.2f}\")\n",
    "    cumulative_rewards.append(total_reward)\n",
    "    episode_lengths.append(episode_length)\n",
    "    rewards.extend([reward] * episode_length)\n",
    "    all_actions.append(actions_episode)\n",
    "    states_episode_all.append(states_episode)\n",
    "    \n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "    if len(agent.memory) == agent.memory.maxlen:\n",
    "        agent.memory.popleft()  # This will remove the oldest experience if memory is full\n",
    "\n",
    "model_save_path = \"C:\\\\Users\\\\varma\\\\OneDrive\\\\Desktop\\\\MastersAI\\\\_MachineLearning\\\\Project\\\\DDQL\\\\Saved_Models\\\\ddql4_model.keras\"\n",
    "agent.model.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3720cb-97d8-4e3a-abd4-009505d9f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Plotting\n",
    "\n",
    "# Plotting total rewards gained in every episode\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cumulative_rewards[-100:])\n",
    "plt.title(\"Learning Curve(Total Rewards per Episode)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Assuming all_actions is a list of lists, where each sublist contains actions for an episode\n",
    "buy_counts = [episode.count('BUY') for episode in all_actions]\n",
    "sell_counts = [episode.count('SELL') for episode in all_actions]\n",
    "hold_counts = [episode.count('HOLD') for episode in all_actions]\n",
    "\n",
    "# stacked bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "episodes = range(len(buy_counts))\n",
    "plt.bar(episodes, buy_counts, label='Buy', color='green')\n",
    "plt.bar(episodes, sell_counts, bottom=buy_counts, label='Sell', color='red')\n",
    "plt.bar(episodes, hold_counts, bottom=[i+j for i,j in zip(buy_counts, sell_counts)], label='Hold', color='blue')\n",
    "\n",
    "plt.title(\"Count of Buy, Sell, Hold Actions per Episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Action Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10847302-c1b3-4d11-844a-b14eebbc3649",
   "metadata": {},
   "source": [
    "# TESTING ENV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ".."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f424aa-d6b7-4a45-91f8-c37e1e39a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "\n",
    "class TradingEnvironmentTest:\n",
    "    def __init__(self, data, look_back):\n",
    "        self.data = data\n",
    "        self.n = len(data)\n",
    "        self.current_step = 0\n",
    "        self.hold_counter = 0\n",
    "        self.hold_penalty = 1  # Initialize the hold penalty factor\n",
    "        self.look_back = look_back  # Add look_back attribute\n",
    "        self.buy_price = -1\n",
    "        self.is_holding = False  # New state variable to track holding position\n",
    "        #self.actions_taken = []\n",
    "\n",
    "    def is_illegal_action(self, action):\n",
    "        # Example condition: cannot sell if not holding a stock\n",
    "        if actions[action] == 'SELL' and self.buy_price == -1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def detect_bullish_engulfing(self):\n",
    "        if self.current_step < 1:\n",
    "            return False\n",
    "    \n",
    "        previous_candle = self.data.iloc[self.current_step - 1]\n",
    "        current_candle = self.data.iloc[self.current_step]\n",
    "    \n",
    "        if (previous_candle['close'] < previous_candle['OPEN'] and\n",
    "                current_candle['close'] > current_candle['OPEN'] and\n",
    "                current_candle['close'] > previous_candle['OPEN'] and\n",
    "                current_candle['OPEN'] < previous_candle['close']):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.look_back  # Start from look_back step\n",
    "        self.buy_price = -1  # Reset buy_price to indicate no stock held\n",
    "        self.is_holding = False  # Reset holding position\n",
    "        return self.data.iloc[self.current_step - self.look_back:self.current_step][features].values\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update the step logic to detect bullish engulfing pattern\n",
    "        if self.detect_bullish_engulfing():\n",
    "            #print(\"Bullish engulfing pattern detected!\")\n",
    "            if self.is_holding:\n",
    "                # Agent is already holding a stock\n",
    "                if actions[action] == 'SELL':\n",
    "                    # Agent decides to sell based on the bullish engulfing pattern\n",
    "                    reward = self.calculate_reward(action)\n",
    "                    next_state = self.data.iloc[self.current_step:self.current_step + self.look_back][features].values\n",
    "                    done = False\n",
    "                else:\n",
    "                    # Agent decides to continue holding\n",
    "                    reward = self.calculate_reward('HOLD')  # Use 'HOLD' directly here\n",
    "                    next_state = self.data.iloc[self.current_step:self.current_step + self.look_back][features].values\n",
    "                    done = False\n",
    "            else:\n",
    "                # Agent is not holding a stock, proceed with regular step logic\n",
    "                reward = self.calculate_reward(action)\n",
    "                self.current_step += 1\n",
    "                if self.current_step + self.look_back > self.n:\n",
    "                    done = True\n",
    "                    next_state = np.zeros((self.look_back, len(features)))  # End of data handling\n",
    "                else:\n",
    "                    done = False\n",
    "                    next_state = self.data.iloc[self.current_step:self.current_step + self.look_back][features].values\n",
    "        else:\n",
    "            if self.is_illegal_action(action):\n",
    "                # Handle illegal action: could be a fixed penalty or other measures\n",
    "                reward = -1000  # Example penalty for illegal action\n",
    "                done = False\n",
    "                next_state = self.data.iloc[self.current_step:self.current_step + self.look_back][features].values\n",
    "            else:\n",
    "                self.current_step += 1\n",
    "                if self.current_step + self.look_back > self.n:\n",
    "                    done = True\n",
    "                    next_state = np.zeros((self.look_back, len(features)))  # End of data handling\n",
    "                else:\n",
    "                    done = False\n",
    "                    next_state = self.data.iloc[self.current_step:self.current_step + self.look_back][features].values\n",
    "                reward = self.calculate_reward(action)\n",
    "    \n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_reward(self, action):\n",
    "        if actions[action] == 'SELL':\n",
    "            if self.buy_price == -1:\n",
    "                # Penalize for selling without holding to prevent shorting\n",
    "                return -1000\n",
    "            else:\n",
    "                profit = self.data.iloc[self.current_step][\"close\"] - self.buy_price\n",
    "                self.buy_price = -1\n",
    "                self.is_holding = False  # Reset holding position after selling\n",
    "                self.hold_counter = 0  # Reset hold counter after selling\n",
    "                return profit\n",
    "    \n",
    "        if actions[action] == 'BUY':\n",
    "            if self.buy_price != -1:\n",
    "                # Small penalty for trying to buy when already holding\n",
    "                return -100\n",
    "            else:\n",
    "                self.buy_price = self.data.iloc[self.current_step][\"OPEN\"]\n",
    "                self.is_holding = True  # Set holding position after buying\n",
    "                return 0  # No immediate reward for buying\n",
    "    \n",
    "        if actions[action] == 'HOLD':\n",
    "            self.hold_counter += 1\n",
    "            # Increasing penalty for holding: the longer you hold, the larger the penalty\n",
    "            penalty = -self.hold_penalty * self.hold_counter * self.hold_counter\n",
    "            return penalty\n",
    "    \n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827451fe-d017-47b5-bfdc-8a40df970584",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnvironmentTest(data, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cca5d-5788-43ec-bab0-e4f2c84a44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified test training code to record only the profit made but not the total reward\n",
    "model = load_model(model_save_path)\n",
    "\n",
    "state_size = len(env.reset()[0])\n",
    "actions = {0: 'SELL', 1: 'BUY', 2: 'HOLD'} \n",
    "\n",
    "batch_size = 10\n",
    "num_episodes = 10\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, agent.look_back, state_size])\n",
    "    total_profit = 0  # To track profit\n",
    "    holding = False   # Flag to track if holding a stock\n",
    "    actions_episode = []\n",
    "\n",
    "    agent.epsilon = 0  # Ensure no random actions during testing\n",
    "\n",
    "    done = False  # Initialize the termination condition\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state, env)\n",
    "        next_state, reward, done = env.step(action)  # Capture the reward value as well\n",
    "    \n",
    "        # Modify the following part accordingly to use the reward value if necessary\n",
    "    \n",
    "        if actions[action] == 'SELL' and env.buy_price != -1:\n",
    "            # Calculate profit for this transaction and reset holding flag\n",
    "            sell_price = env.data.iloc[env.current_step][\"close\"]\n",
    "            profit = sell_price - env.buy_price\n",
    "            total_profit += profit\n",
    "            holding = False\n",
    "    \n",
    "        elif actions[action] == 'BUY':\n",
    "            # Set holding flag to true when buying\n",
    "            holding = True\n",
    "    \n",
    "        state = np.reshape(next_state, [1, agent.look_back, state_size])\n",
    "        actions_episode.append(actions[action])\n",
    "    \n",
    "    holding_status = \"Holding\" if holding else \"Not Holding\"\n",
    "    print(f\"Episode: {episode + 1}/{num_episodes}, Total Profit: {total_profit:.2f}, {holding_status}\")\n",
    "    all_actions.append(actions_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a16bb-197e-4e3f-9bc8-abd685a2c833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
